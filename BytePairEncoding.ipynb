{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvDiN+Gu6YAkNnWbTYM4jn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from collections import defaultdict\n","from typing import Dict, List, Tuple, Set\n","\n","def get_stats(vocab_words: List[str]) -> Dict[Tuple[str, str], int]:\n","    \"\"\"\n","    Count frequency of adjacent character pairs across all words.\n","    \"\"\"\n","    pairs = defaultdict(int)\n","    for word in vocab_words:\n","        symbols = word.split()\n","        for i in range(len(symbols)-1):\n","            pairs[symbols[i], symbols[i+1]] += 1\n","    return pairs\n","\n","def merge_vocab(pair: Tuple[str, str], vocab_words: List[str]) -> List[str]:\n","    \"\"\"\n","    Merge all occurrences of the most frequent pair in each word.\n","    \"\"\"\n","    merged = pair[0] + pair[1]\n","    new_vocab_words = []\n","    for word in vocab_words:\n","        symbols = word.split()\n","        i = 0\n","        new_symbols = []\n","        while i < len(symbols):\n","            if i < len(symbols) - 1 and symbols[i] == pair[0] and symbols[i + 1] == pair[1]:\n","                new_symbols.append(merged)\n","                i += 2\n","            else:\n","                new_symbols.append(symbols[i])\n","                i += 1\n","        new_vocab_words.append(' '.join(new_symbols))\n","    return new_vocab_words\n","\n","def get_unique_tokens(vocab_words: List[str]) -> Set[str]:\n","    \"\"\"\n","    Get set of unique tokens from space-separated words.\n","    \"\"\"\n","    tokens = set()\n","    for word in vocab_words:\n","        tokens.update(word.split())\n","    return tokens\n","\n","def byte_pair_encoding(text: str, num_merges: int = 10) -> List[str]:\n","    \"\"\"\n","    Perform byte pair encoding on input text.\n","    \"\"\"\n","    # Initialize vocabulary with space-separated characters\n","    vocab_words = [' '.join(word) for word in text.replace(',', '').split()]\n","\n","    print(\"Initial vocabulary:\")\n","    tokens = get_unique_tokens(vocab_words)\n","    print(', '.join(sorted(tokens)))\n","    print()\n","\n","    for i in range(num_merges):\n","        # Get statistics for pairs\n","        pairs = get_stats(vocab_words)\n","        if not pairs:\n","            break\n","\n","        # Find most frequent pair\n","        most_freq = max(pairs.items(), key=lambda x: (x[1], x[0]))[0]\n","        merged = most_freq[0] + most_freq[1]\n","\n","        print(f\"Step {i+1}:\")\n","        print(f\"Most frequent pair: ({most_freq[0]}, {most_freq[1]}) with frequency {pairs[most_freq]}\")\n","        print(f\"Merging into: {merged}\")\n","\n","        # Merge the most frequent pair in vocabulary\n","        vocab_words = merge_vocab(most_freq, vocab_words)\n","\n","        print(\"\\nVocabulary:\")\n","        tokens = get_unique_tokens(vocab_words)\n","        print(', '.join(sorted(tokens)))\n","        print()\n","\n","    return vocab_words\n","\n","# Run BPE on the example text\n","text = \"fred fed ted bread and ted fed fred bread\"\n","print(\"Original text:\", text)\n","print()\n","final_vocab = byte_pair_encoding(text, num_merges=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-0FNQbpLVqs","executionInfo":{"status":"ok","timestamp":1730020853634,"user_tz":-330,"elapsed":404,"user":{"displayName":"Elijah Hoole","userId":"00743769831414297941"}},"outputId":"07033e4d-1659-4ce8-88ac-ddddad44a856"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text: fred fed ted bread and ted fed fred bread\n","\n","Initial vocabulary:\n","a, b, d, e, f, n, r, t\n","\n","Step 1:\n","Most frequent pair: (e, d) with frequency 6\n","Merging into: ed\n","\n","Vocabulary:\n","a, b, d, e, ed, f, n, r, t\n","\n","Step 2:\n","Most frequent pair: (t, ed) with frequency 2\n","Merging into: ted\n","\n","Vocabulary:\n","a, b, d, e, ed, f, n, r, ted\n","\n","Step 3:\n","Most frequent pair: (r, ed) with frequency 2\n","Merging into: red\n","\n","Vocabulary:\n","a, b, d, e, ed, f, n, r, red, ted\n","\n","Step 4:\n","Most frequent pair: (r, e) with frequency 2\n","Merging into: re\n","\n","Vocabulary:\n","a, b, d, ed, f, n, re, red, ted\n","\n","Step 5:\n","Most frequent pair: (re, a) with frequency 2\n","Merging into: rea\n","\n","Vocabulary:\n","a, b, d, ed, f, n, rea, red, ted\n","\n"]}]},{"cell_type":"markdown","source":["f r ed / f ed / ted / b r e a d / a n d/  ted / f ed / f r ed / b r e a d"],"metadata":{"id":"oscbDlkSWSyX"}},{"cell_type":"code","source":["def learn_bpe_merges(text: str, num_merges: int = 5) -> List[Tuple[str, str]]:\n","    \"\"\"Learn BPE merge operations from training text.\"\"\"\n","    # Initialize vocabulary with space-separated characters\n","    vocab_words = [' '.join(word) for word in text.replace(',', '').split()]\n","    merges = []\n","\n","    for i in range(num_merges):\n","        # Get statistics for pairs\n","        pairs = defaultdict(int)\n","        for word in vocab_words:\n","            symbols = word.split()\n","            for j in range(len(symbols)-1):\n","                pairs[symbols[j], symbols[j+1]] += 1\n","\n","        if not pairs:\n","            break\n","\n","        # Find most frequent pair\n","        most_freq = max(pairs.items(), key=lambda x: (x[1], x[0]))[0]\n","        merges.append(most_freq)\n","\n","        # Merge the pair in vocabulary\n","        vocab_words = merge_vocab(most_freq, vocab_words)\n","\n","    return merges\n","\n","def tokenize_word(word: str, merges: List[Tuple[str, str]]) -> List[str]:\n","    \"\"\"\n","    Tokenize a single word using learned BPE merges.\n","    \"\"\"\n","    # Start with characters separated\n","    word = ' '.join(list(word))\n","\n","    # Apply merges in order\n","    for pair in merges:\n","        new_token = pair[0] + pair[1]\n","        symbols = word.split()\n","        i = 0\n","        new_symbols = []\n","        while i < len(symbols):\n","            if i < len(symbols) - 1 and symbols[i] == pair[0] and symbols[i + 1] == pair[1]:\n","                new_symbols.append(new_token)\n","                i += 2\n","            else:\n","                new_symbols.append(symbols[i])\n","                i += 1\n","        word = ' '.join(new_symbols)\n","\n","    return word.split()\n","\n","# First, train BPE on original text\n","training_text = \"fred fed ted bread and ted fed fred bread\"\n","print(\"Training text:\", training_text)\n","print()\n","\n","# Learn merges\n","merges = learn_bpe_merges(training_text, num_merges=5)\n","\n","# Print the learned merge operations\n","print(\"Learned BPE merge operations:\")\n","for i, (a, b) in enumerate(merges, 1):\n","    print(f\"{i}. Merge '{a}' + '{b}' → '{a+b}'\")\n","print()\n","\n","# Example new sentences to tokenize\n","test_sentences = [\n","    \"ted freed bread\",\n","    \"red feed\",\n","    \"breed\"\n","]\n","\n","# Tokenize each test sentence\n","print(\"Tokenizing new sentences:\")\n","for sentence in test_sentences:\n","    print(f\"\\nOriginal: {sentence}\")\n","    tokens = []\n","    for word in sentence.split():\n","        word_tokens = tokenize_word(word, merges)\n","        tokens.extend(word_tokens)\n","    print(\"Tokens:\", tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D0mAtWvkQPIX","executionInfo":{"status":"ok","timestamp":1730020972421,"user_tz":-330,"elapsed":516,"user":{"displayName":"Elijah Hoole","userId":"00743769831414297941"}},"outputId":"ac7acd3a-da14-4847-82c0-0b8cdae82ddb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Training text: fred fed ted bread and ted fed fred bread\n","\n","Learned BPE merge operations:\n","1. Merge 'e' + 'd' → 'ed'\n","2. Merge 't' + 'ed' → 'ted'\n","3. Merge 'r' + 'ed' → 'red'\n","4. Merge 'r' + 'e' → 're'\n","5. Merge 're' + 'a' → 'rea'\n","\n","Tokenizing new sentences:\n","\n","Original: ted freed bread\n","Tokens: ['ted', 'f', 're', 'ed', 'b', 'rea', 'd']\n","\n","Original: red feed\n","Tokens: ['red', 'f', 'e', 'ed']\n","\n","Original: breed\n","Tokens: ['b', 're', 'ed']\n"]}]},{"cell_type":"markdown","source":["ted / f re ed / b rea d"],"metadata":{"id":"e4Dnneb0XjBr"}},{"cell_type":"code","source":["                X,    y\n","\n","      image1,        class of the image\n","      image2,        class of the image\n","      image3,        class of the image\n","      image4,        class of the image\n","      image5,        class of the image\n","\n","\n","\n","               x,  y\n","  I love you       Je vous aime\n","  I hate you       I hate you in french\n","\n","              X,   y\n"," I [MASK] you        I love you\n","\n","\n"],"metadata":{"id":"7bQ5j6SUSRL_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["X                                               y  \n","Suddenly, [MASK] have serious [MASK] options.   Suddenly, Sri Lanka have serious batting options.\n","\n","\n","It was only in [MASK] that they crashed out of the T20 World Cup after an exceptionally dire showing in their first two matches.      It was only in July that they crashed out of the T20 World Cup after an exceptionally showing in their first two matches.\n","\n","They'd stunk up the [MASK] World Cup even worse in late 2023, failing to finish among the top eight, and as such, out on qualification for the Champions Trophy for the first time.    They'd stunk up the ODI World Cup even worse in late 2023, failing to finish among the top eight, and as such, missing out on qualification for the Champions Trophy for the first time.\n"],"metadata":{"id":"PiI82DQudvTZ"}},{"cell_type":"code","source":[],"metadata":{"id":"aeTXGQkqeDYP"},"execution_count":null,"outputs":[]}]}